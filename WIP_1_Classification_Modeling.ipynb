{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logist regression .Initating the scaler \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting the scaler\n",
    "Xss=scaler.fit_transform(X)\n",
    "\n",
    "logreg = LogisticRegression(C=10**10,solver='lbfgs')\n",
    "logreg.fit(Xss, high_low)\n",
    "\n",
    "print('Logreg intercept:', logreg.intercept_)\n",
    "print('Logreg coef(s):', logreg.coef_)\n",
    "print('Logreg predicted probabilities:\\n', logreg.predict_proba(X.iloc[0:5,:]))\n",
    "print('Logreg predicted classes:', logreg.predict(X.iloc[0:5,:]))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(logreg,Xss, high_low,cv=10)\n",
    "print(scores)\n",
    "print(np.mean(scores))\n",
    "\n",
    "# The accuracy of the model\n",
    "np.mean(scores)\n",
    "\n",
    "print('Logreg predicted labels:\\n',logreg.predict(X)[:500])\n",
    "\n",
    "# Putting the coefs into a df and sorting \n",
    "coef_df = pd.DataFrame(data=logreg.coef_[0],index=X.columns)\n",
    "coef_df['abs'] = np.abs(coef_df[0])\n",
    "coef_df.sort_values(by=\"abs\",ascending=False,inplace=True)\n",
    "\n",
    "# Viewing the coefs in order of impact \n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "coef_df['abs'].plot(kind=\"barh\",ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# Viewing coefs in order of importance and how the effect the model.\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "coef_df[0].plot(kind=\"barh\",ax=ax)\n",
    "plt.show()\n",
    "\n",
    "coef_df\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "logreg.predict(Xss).shape\n",
    "\n",
    "# Viewing the confusion matrix \n",
    "confusion_matrix = metrics.confusion_matrix(high_low, logreg.predict(Xss))\n",
    "print(metrics.classification_report(high_low, logreg.predict(Xss)))\n",
    "\n",
    "# viewing the precion scores \n",
    "precision_scores = metrics.precision_score(high_low,logreg.predict(Xss), average=None)\n",
    "print(precision_scores)\n",
    "print(\"Macro:\",metrics.precision_score(high_low,logreg.predict(Xss), average='macro'))\n",
    "print(\"Micro:\",metrics.precision_score(high_low,logreg.predict(Xss), average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting precion-recall curve\n",
    "y_bin = label_binarize(high_low, logreg.classes_)\n",
    "y_bin[:10]\n",
    "\n",
    "def plot_precision_recall(model, y_bin, X, f1_lines=True):\n",
    "    '''\n",
    "    Takes a fitted model, \n",
    "    an array of binarised true y-values, \n",
    "    a predictor matrix and\n",
    "    and additional argument if lines of constant f1-score levels should be plotted.\n",
    "    Returns a precision-recall plot for each of the classes.\n",
    "    '''\n",
    "\n",
    "    # create dictionaries for storing precision/recall values\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "\n",
    "    # loop over all the classes\n",
    "    # calculate precision-recall values for the current class versus the rest\n",
    "    for i, cl in enumerate(model.classes_):\n",
    "        precision[cl], recall[cl], threshold = metrics.precision_recall_curve(\n",
    "            y_bin[:, i], model.predict_proba(X)[:, i])\n",
    "\n",
    "    # average_precision_score calculates the area under the curve for each class\n",
    "    average_precision_scores = [round(metrics.average_precision_score(\n",
    "        y_bin[:, i], model.predict_proba(X)[:, i]), 2) for i in range(len(model.classes_))]\n",
    "\n",
    "    # plot the precision-recall curves for each class versus the rest\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    for i, key in enumerate(precision.keys()):\n",
    "        ax.plot(precision[key], recall[key], lw=2,\n",
    "                 label='class {}, average precision score {}'.format(key, average_precision_scores[i]))\n",
    "\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.legend(loc=[1,0])\n",
    "    ax.set_title('Precision-Recall Curve', fontsize=20)\n",
    "    ax.set_xlabel('Recall', fontsize=18)\n",
    "    ax.set_ylabel('Precision', fontsize=18)\n",
    "\n",
    "    # add lines of constant F1 scores\n",
    "    if f1_lines == True:\n",
    "        for const in np.linspace(0.2,0.9,8):\n",
    "            x_vals = np.linspace(0.001, 0.999, 100)\n",
    "            y_vals = 1./(2./const-1./x_vals)\n",
    "            ax.plot(x_vals[y_vals > 0], y_vals[y_vals > 0],\n",
    "                     color='lightblue', ls='--', alpha=0.9)\n",
    "            ax.set_ylim([0, 1])\n",
    "            ax.annotate('f1={0:0.1f}'.format(const),\n",
    "                         xy=(x_vals[-10], y_vals[-2]+0.0))\n",
    "\n",
    "    return fig, ax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
